{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "import mwapi\n",
    "import time\n",
    "import sys\n",
    "import mwparserfromhell as mwparser\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from scipy.sparse import coo_matrix, vstack, hstack\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import hashlib\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def fix_data_type(i):\n",
    "    if i == 'True':\n",
    "        return True\n",
    "    if i == 'False':\n",
    "        return False\n",
    "    if i.isdigit() == True:\n",
    "        return int(i)\n",
    "    else:\n",
    "        return float(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_pageid(doc):\n",
    "    pageid = doc['query']['pages'].keys()\n",
    "    pageid = list(pageid)[0]\n",
    "    return pageid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_parent_revid(doc):\n",
    "    parent_revid = doc['query']['pages'][str(get_pageid(doc))]['revisions'][0]['parentid']\n",
    "    return parent_revid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_contents(revid):\n",
    "    \"\"\"\n",
    "    get content for revid and it's parent\n",
    "    /w/api.php?action=query&format=json&prop=revisions&pageids=5887233&rvprop=ids%7Ccontent&rvlimit=2&rvstartid=655710468\n",
    "    \"\"\"\n",
    "\n",
    "    session = mwapi.Session('https://en.wikipedia.org',\n",
    "                            user_agent='Hashing vectorizer P.O.C <ruj.sabya@gmail.com>, <aaron.halfaker@gmail.com>')\n",
    "\n",
    "    try:\n",
    "        doc = session.get(action='query', prop='revisions', revids=[revid], rvprop=['ids'])\n",
    "    except:\n",
    "        print(\"Error: \", sys.exc_info()[0])\n",
    "        return False\n",
    "\n",
    "    if 'badrevids' in doc['query']:\n",
    "        # if the page related to the revision is deleted\n",
    "        print (\"bad revid: %s\" % (revid))\n",
    "        return False\n",
    "\n",
    "    pageid = get_pageid(doc)\n",
    "    parent_revid = get_parent_revid(doc)\n",
    "    print(\"Pageid, parent revid = \", (pageid, parent_revid))\n",
    "\n",
    "    try:\n",
    "        doc = session.get(action='query',\n",
    "                          prop='revisions',\n",
    "                          pageids=[pageid],\n",
    "                          rvprop=['ids', 'content'],\n",
    "                          rvlimit=2,\n",
    "                          rvstartid=[revid],\n",
    "                          rvdir='older')\n",
    "    except:\n",
    "        print(\"Error: \", sys.exc_info()[0])\n",
    "        return False\n",
    "\n",
    "    pp.pprint((pageid, revid))\n",
    "    current_text = doc['query']['pages'][str(pageid)]['revisions'][0]['*']\n",
    "    parent_text = doc['query']['pages'][str(pageid)]['revisions'][1]['*'] if parent_revid else ''\n",
    "\n",
    "    result = {'revid':revid,\n",
    "              'revid_parent': parent_revid,\n",
    "              'parent': parent_text,\n",
    "              'current': current_text}\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def read_tsv(fileobj):\n",
    "    tsvin = csv.reader(fileobj, delimiter='\\t')\n",
    "    for row in tsvin:\n",
    "        yield row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def open_db(db_name = 'data.db'):\n",
    "    conn = sqlite3.connect(db_name)\n",
    "    conn.isolation_level = None;\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_sqlite_tables():\n",
    "    # source db\n",
    "    conn = open_db()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS observations\n",
    "    (revid INTEGER PRIMARY KEY, other_features TEXT, is_damaging INTEGER)''')\n",
    "\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS content\n",
    "    (revid INTEGER PRIMARY KEY, revid_parent INTEGER, content_current BLOB, content_parent BLOB)''')\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "    # features db\n",
    "    conn = open_db('features.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS feature_vector\n",
    "    (revid INTEGER PRIMARY KEY, current BLOB, parent BLOB, diff BLOB, other_features BLOB, is_damaging INTEGER)''')\n",
    "    conn.close()\n",
    "\n",
    "    # score db\n",
    "    # TODO - verify if we are populating other_features if features.db\n",
    "    # is newly being created\n",
    "    conn = open_db('score.db')\n",
    "    c = conn.cursor()\n",
    "    c.execute('''CREATE TABLE IF NOT EXISTS score\n",
    "    (revid INTEGER PRIMARY KEY, is_damaging_actual INTEGER, is_damaging_prediction INTEGER, score_positive REAL)''')\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def copy_other_features_to_features_db():\n",
    "    conn_source = open_db();\n",
    "    conn_features = open_db('features.db')\n",
    "    cs = conn_source.cursor()\n",
    "    cf = conn_features.cursor()\n",
    "\n",
    "    ret = cs.execute('''SELECT revid, other_features FROM observations''')\n",
    "    print('hi')\n",
    "    for row in ret:\n",
    "        msg = \"inserting other features for: \" + str(row[0]) + \"\\r\\r\"\n",
    "        print(msg, end='\\r')\n",
    "        other_features = pickle.dumps(map(fix_data_type, pickle.loads(row[1])))\n",
    "        #print(list(pickle.loads(other_features)))\n",
    "        ret = cf.execute('''UPDATE feature_vector SET other_features=? WHERE revid = ?''',(other_features, row[0]))\n",
    "    conn_source.close()\n",
    "    conn_features.close()\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def export_tsv_to_sqlite():\n",
    "    create_sqlite_tables()\n",
    "    conn = open_db()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    filename = 'enwiki.features_damaging.20k_2015.tsv'\n",
    "    f = open(filename,'rt')\n",
    "    i = 1\n",
    "    for row in read_tsv(f):\n",
    "        print(i)\n",
    "        i = i + 1\n",
    "        other_features = pickle.dumps(row[1:-1]) #map(fix_data_type, row[1:-1])\n",
    "        # other_features = pickle.dumps(map(fix_data_type, row[1:-1]))\n",
    "        c.execute('''INSERT INTO observations\n",
    "        (revid, other_features, is_damaging)\n",
    "        VALUES (?, ?, ?)''', (row[0], other_features, row[-1]))\n",
    "    conn.commit()\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def download_conents():\n",
    "    create_sqlite_tables()\n",
    "    conn = open_db()\n",
    "    c = conn.cursor()\n",
    "\n",
    "    # read from sqlite\n",
    "    ret = c.execute('''SELECT revid FROM observations WHERE revid NOT IN (SELECT revid FROM content)''')\n",
    "    i = 1\n",
    "\n",
    "    ci = conn.cursor()\n",
    "    for row in ret:\n",
    "        revid = row[0]\n",
    "        print(i, revid)\n",
    "        i = i + 1\n",
    "        content = get_contents(revid)\n",
    "\n",
    "        if content == False:\n",
    "            ci.execute('''INSERT INTO content(revid) VALUES (?)''', (revid,))\n",
    "            continue\n",
    "\n",
    "        ci.execute('''INSERT INTO\n",
    "          content(revid, revid_parent, content_current, content_parent)\n",
    "          VALUES (?, ?, ?, ?)''',\n",
    "                        (content['revid'],\n",
    "                         content['revid_parent'],\n",
    "                         content['current'],\n",
    "                         content['parent']\n",
    "                        ))\n",
    "\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def extract_features():\n",
    "    hv = HashingVectorizer(n_features=2 ** 20, ngram_range=(1, 3))\n",
    "    conn_source = open_db();\n",
    "    conn_features = open_db('features.db')\n",
    "\n",
    "    cs = conn_source.cursor()\n",
    "    cf = conn_features.cursor()\n",
    "\n",
    "    ret = cs.execute('''SELECT\n",
    "     content.revid, content_current, content_parent, other_features, is_damaging\n",
    "    FROM\n",
    "     content INNER JOIN observations ON content.revid=observations.revid\n",
    "    WHERE content.revid_parent IS NOT NULL''')\n",
    "\n",
    "    #TODO - here we need to insert other_features instead of\n",
    "    # copy_other_features_to_features_db\n",
    "    for row in ret:\n",
    "        print(\"inserting features for\", (row[0]))\n",
    "        features = hv.transform((row[1], row[2]))\n",
    "        features_diff = features[0] - features[1]\n",
    "        ret = cf.execute('''INSERT INTO feature_vector(revid, current, parent, diff, is_damaging) VALUES (?, ?, ?, ?, ?)''' ,\n",
    "                         (\n",
    "                             row[0],\n",
    "                             pickle.dumps(features[0]),\n",
    "                             pickle.dumps(features[1]),\n",
    "                             pickle.dumps(features_diff),\n",
    "                             row[4]\n",
    "                         ))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_features_labels_training(consider_other_features = True, hv_features_threshold = 0):\n",
    "    conn_features = open_db('features.db')\n",
    "    cf = conn_features.cursor()\n",
    "\n",
    "    ret = cf.execute('''SELECT other_features, diff, is_damaging FROM feature_vector ORDER BY revid LIMIT 16000''')\n",
    "\n",
    "    print('fetching')\n",
    "    rows = ret.fetchall()\n",
    "    conn_features.close()\n",
    "\n",
    "    print('zipping')\n",
    "    other_features, features_vector, labels = zip(*rows)\n",
    "\n",
    "    count = 0\n",
    "    print('unpickling')\n",
    "    features = coo_matrix([])\n",
    "\n",
    "    for i in features_vector:\n",
    "        print(count)\n",
    "        hv_features = pickle.loads(i)\n",
    "\n",
    "        vector = hv_features\n",
    "        other_features_coo = coo_matrix([])\n",
    "        if consider_other_features == True:\n",
    "            other_features_coo = pickle.loads(other_features[count])\n",
    "            other_features_coo = list(map(fix_data_type, other_features_coo))\n",
    "            other_features_coo = coo_matrix([other_features_coo])\n",
    "            vector = hstack([other_features_coo, hv_features])\n",
    "\n",
    "        if features.getnnz() == 0:\n",
    "            features = vstack([vector])\n",
    "        else:\n",
    "            features = vstack([features, vector])\n",
    "            count = count + 1\n",
    "\n",
    "    print('saving vstacked features')\n",
    "    joblib.dump(features, 'model_pickled/training_data_features.pkl')\n",
    "    joblib.dump(labels, 'model_pickled/training_data_labels.pkl')\n",
    "    return features,labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def build_model_wihout_other_features():\n",
    "    features, labels = fetch_hv_features_with_labels(TRAINING_QUERY_HV_FEATURES)        \n",
    "\n",
    "    print('Shape of features:', features.shape)\n",
    "    print('Length of labels:', len(labels))\n",
    "\n",
    "    print('fitting')\n",
    "    gbc = GradientBoostingClassifier(n_estimators=700, max_depth=7, learning_rate=0.01)\n",
    "    sample_weight=[18939 / (796 + 18939) if l == 'True' else 796 / (796 + 18939) for l in labels]\n",
    "    gbc.fit(features, labels, sample_weight)\n",
    "\n",
    "    print('saving')\n",
    "    joblib.dump(gbc, 'model_pickled/gbc.pkl')\n",
    "    return gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def example_predictions():\n",
    "    features = joblib.load('model_pickled/features_score.pkl')\n",
    "    rows = joblib.load('model_pickled/rows_score.pkl')\n",
    "    gbc = joblib.load('model_pickled/gbc.pkl')\n",
    "\n",
    "    features_vector, labels = zip(*rows)\n",
    "    del features_vector\n",
    "    for i in range(features.shape[0]):\n",
    "        print('Actual: ', labels[i], 'Prediction: ', dict(zip(gbc.classes_,\n",
    "                                                              [int(v*100)\n",
    "                                                              for v in\n",
    "                                                              gbc.predict_proba(features.getrow(i).todense())[0]])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# def build_model_with_selected_hash(consider_other_features):\n",
    "#     try:\n",
    "#         features = joblib.load('model_pickled/training_data_features.pkl')\n",
    "#         labels = joblib.load('model_pickled/training_data_labels.pkl')\n",
    "#     except FileNotFoundError:\n",
    "#         features, labels = get_features_labels_training(consider_other_features)\n",
    "\n",
    "#     print('fitting')\n",
    "#     gbc = GradientBoostingClassifier(n_estimators=200, learning_rate=0.05)\n",
    "#     sample_weight=[18939 / (796 + 18939) if l == 'True' else 796 / (796 + 18939) for l in labels]\n",
    "#     gbc.fit(features, labels, sample_weight)\n",
    "\n",
    "#     print('saving')\n",
    "#     joblib.dump(gbc, 'model_pickled/gbc_selected_hash.pkl')\n",
    "#     return gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# fetches hv_features from feature_vector table using query\n",
    "# sample query is: SELECT diff, is_damaging FROM feature_vector where revid > X ORDER BY revid LIMIT Y\n",
    "def fetch_hv_features_with_labels(query):\n",
    "    uidsave = hashlib.md5(query.encode('utf-8')).hexdigest()\n",
    "    filename_hv_features = 'model_pickled/hv_features_' + uidsave + '.pkl'\n",
    "    filename_labels = 'model_pickled/labels_' + uidsave + '.pkl'\n",
    "\n",
    "    try:\n",
    "        print('trying to load pickle files:\\n  {0}\\n  {1}'.format(filename_hv_features, filename_labels))\n",
    "        features = joblib.load(filename_hv_features)\n",
    "        labels = joblib.load(filename_labels)\n",
    "    except FileNotFoundError:\n",
    "        print('Pickle files not found. Creating.')\n",
    "        conn_features = open_db('features.db')\n",
    "        cf = conn_features.cursor()\n",
    "        ret = cf.execute(query)\n",
    "\n",
    "        print('fetchall')\n",
    "        rows = ret.fetchall()\n",
    "        conn_features.close()\n",
    "    \n",
    "        print('zipping')\n",
    "        features_vector, labels = zip(*rows)\n",
    "\n",
    "        count = 0\n",
    "        print('unpickling and stacking')\n",
    "        features = coo_matrix([])\n",
    "\n",
    "        for i in features_vector:\n",
    "            print(count, end='\\r');sys.stdout.flush();\n",
    "            hv_features = pickle.loads(i)\n",
    "\n",
    "            vector = hv_features\n",
    "            if features.getnnz() == 0:\n",
    "                features = vstack([vector])\n",
    "            else:\n",
    "                features = vstack([features, vector])\n",
    "                count = count + 1\n",
    "\n",
    "        print('Saving')\n",
    "        joblib.dump(features, filename_hv_features)\n",
    "        joblib.dump(labels, filename_labels)\n",
    "    labels = list(map(fix_data_type, labels))\n",
    "    return features,labels\n",
    "\n",
    "def select_hv_features(all_hv_features, model, threshold):\n",
    "    sfm = SelectFromModel(model, threshold, prefit=True)\n",
    "    features = sfm.get_support(indices=True)\n",
    "    print('Length of get_support:', len(features))\n",
    "    selected_hv_features = sfm.transform(all_hv_features)\n",
    "    selected_hv_features = coo_matrix(selected_hv_features)\n",
    "    return selected_hv_features\n",
    "\n",
    "# sample query: 'SELECT other_features FROM feature_vector where revid > X ORDER BY revid LIMIT T'\n",
    "def fetch_other_features(query):\n",
    "    conn_features = open_db('features.db')\n",
    "    cf = conn_features.cursor()\n",
    "    ret = cf.execute(query)\n",
    "    rows = ret.fetchall()\n",
    "    features = []\n",
    "    count = 0\n",
    "    for i in rows:\n",
    "        features.append(list(pickle.loads(i[0])))\n",
    "        count = count + 1\n",
    "    features = coo_matrix(features)\n",
    "    return features\n",
    "\n",
    "def build_model_and_save(features, labels, pickle_prefix = ''):\n",
    "    #TODO load from saved\n",
    "    print('fitting')\n",
    "    gbc = GradientBoostingClassifier(n_estimators=700, max_depth=7, learning_rate=0.01)\n",
    "    sample_weight=[18939 / (796 + 18939) if l == 'True' else 796 / (796 + 18939) for l in labels]\n",
    "    gbc.fit(features, labels, sample_weight)\n",
    "    # print('saving')    #TODO\n",
    "    return gbc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def process_features():\n",
    "    # -- hv_features\n",
    "    hv_features, labels = fetch_hv_features_with_labels(TRAINING_QUERY_HV_FEATURES)\n",
    "    # -- other_features\n",
    "    other_features = fetch_other_features(TRAINING_QUERY_OTHER_FEATURES)\n",
    "\n",
    "    # -- hv_features\n",
    "    hv_features, labels = fetch_hv_features_with_labels(SCORE_QUERY_HV_FEATURES)\n",
    "    # -- other_features\n",
    "    other_features = fetch_other_features(SCORE_QUERY_OTHER_FEATURES)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def process_build_model(threshold):\n",
    "    uidsave = hashlib.md5(str(threshold).encode('utf-8')).hexdigest()\n",
    "    filename_model = 'model_pickled/model_threshold_' + VERSION_MODEL + '_'   + uidsave + '.pkl'\n",
    "    \n",
    "    try:\n",
    "        print('trying to load pickle files:\\n  {0}'.format(filename_model))\n",
    "        gbc = joblib.load(filename_model)\n",
    "    except FileNotFoundError:\n",
    "        print('Not found. Creating..')    \n",
    "        #consider_other_features = False\n",
    "        model = joblib.load('model_pickled/gbc.pkl')\n",
    "        pickle_prefix = str(consider_other_features) + '_' + str(hv_features_threshold) + '_'\n",
    "\n",
    "        # -- hv_features\n",
    "        hv_features, labels = fetch_hv_features_with_labels(TRAINING_QUERY_HV_FEATURES)        \n",
    "        # -- other_features\n",
    "        other_features = fetch_other_features(TRAINING_QUERY_OTHER_FEATURES)\n",
    "\n",
    "        # -- select hv_features and combine with other_features    \n",
    "        selected_hv_features = select_hv_features(hv_features, model, threshold)\n",
    "        \n",
    "        combined_features = other_features\n",
    "        if (selected_hv_features.shape[1] > 0):            \n",
    "            combined_features = hstack([other_features, selected_hv_features])\n",
    "        print('Shape of combined features: ', combined_features.shape)\n",
    "\n",
    "        # create model with combined features\n",
    "        gbc = build_model_and_save(combined_features, labels, pickle_prefix)\n",
    "        joblib.dump(gbc, filename_model)\n",
    "    return gbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def clean_score_table():\n",
    "    conn_score = open_db('score.db')\n",
    "    cc = conn_score.cursor()\n",
    "    cc.execute('''DELETE FROM score''')\n",
    "    conn_score.close()   \n",
    "\n",
    "def score_model_iterative(model, revids, features, original_labels):\n",
    "    # could not use score method, because when i convert the sparse feature\n",
    "    # matrix to dense python throws MemoryError\n",
    "\n",
    "    conn_score = open_db('score.db')\n",
    "    cc = conn_score.cursor()\n",
    "\n",
    "    count = 0\n",
    "    for row in features.todense():\n",
    "        prediction = model.predict(row)[0]\n",
    "        score_positive = model.predict_proba(row)[0][1]\n",
    "        classes = model.classes_\n",
    "        #print('revid: ', revids[count], ', actual: ', original_labels[count], ', prediction', prediction, 'score_positive', score_positive, 'classes', classes)        \n",
    "        #print('types: prediction', type(prediction), 'original_label:', type(original_labels[count]))\n",
    "        print('scored till: ', count, end='\\r')        \n",
    "        \n",
    "        cc.execute('''\n",
    "                    INSERT INTO score\n",
    "                    (revid, is_damaging_actual, is_damaging_prediction, score_positive)\n",
    "                    VALUES (?, ?, ?, ?)''', \n",
    "                    (int(revids[count]), original_labels[count], bool(prediction), score_positive))\n",
    "        \n",
    "        \n",
    "        count = count + 1\n",
    "    conn_score.close()\n",
    "\n",
    "        \n",
    "\n",
    "def calculate_performance_stats():\n",
    "    conn_score = open_db('score.db')\n",
    "    cc = conn_score.cursor()\n",
    "\n",
    "    # calculate score\n",
    "    correct_predictions =  cc.execute('''SELECT COUNT(revid) FROM score WHERE is_damaging_actual=is_damaging_prediction''').fetchone()[0]\n",
    "    total_predictions   =  cc.execute('''SELECT COUNT(revid) FROM score''').fetchone()[0]\n",
    "    score = (correct_predictions/total_predictions) * 100\n",
    "    print('Correct Predictions: ', correct_predictions, 'Total Predictions: ', total_predictions, 'Score: ', score)\n",
    "\n",
    "    # calculate PR-AUC\n",
    "    rows =  cc.execute('''SELECT is_damaging_actual, score_positive FROM score''').fetchall()\n",
    "    y_true, y_scores = zip(*rows)\n",
    "    y_true = [True if i==True else False for i in y_true]\n",
    "\n",
    "    avg_pre = average_precision_score(y_true, list(y_scores))\n",
    "    roc_auc = roc_auc_score(y_true, list(y_scores))\n",
    "\n",
    "    print('average precision score: ', avg_pre, 'roc auc score: ', roc_auc)\n",
    "\n",
    "    #cleanup\n",
    "    conn_score.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def process_evaluate(gbc, threshold):\n",
    "    # -- hv_features\n",
    "    hv_features, labels = fetch_hv_features_with_labels(SCORE_QUERY_HV_FEATURES)\n",
    "    # -- other_features\n",
    "    other_features = fetch_other_features(SCORE_QUERY_OTHER_FEATURES)\n",
    "    \n",
    "    # -- select hv_features and combine with other_features  \n",
    "    model = joblib.load('model_pickled/gbc.pkl')    \n",
    "    selected_hv_features = select_hv_features(hv_features, model, threshold)\n",
    "    \n",
    "    combined_features = other_features\n",
    "    if (selected_hv_features.shape[1] > 0):\n",
    "        combined_features = hstack([other_features, selected_hv_features])\n",
    "    \n",
    "    #get revids\n",
    "    conn_features = open_db('features.db')\n",
    "    cf = conn_features.cursor()\n",
    "    ret = cf.execute(SCORE_REVIDS)\n",
    "    rows = ret.fetchall()\n",
    "    rows = np.array(rows)\n",
    "    revids = rows[:,0]\n",
    "    \n",
    "    #score\n",
    "    score_model_iterative(gbc, revids, combined_features, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "TRAINING_QUERY_OTHER_FEATURES = '''SELECT other_features FROM feature_vector ORDER BY revid LIMIT 16000'''\n",
    "TRAINING_QUERY_HV_FEATURES    = '''SELECT diff, is_damaging FROM feature_vector ORDER BY revid LIMIT 16000'''\n",
    "\n",
    "SCORE_QUERY_OTHER_FEATURES    = '''SELECT other_features FROM feature_vector WHERE revid > 646706890 ORDER BY revid LIMIT 4000'''\n",
    "SCORE_QUERY_HV_FEATURES       = '''SELECT diff, is_damaging FROM feature_vector WHERE revid > 646706890 ORDER BY revid LIMIT 4000'''\n",
    "\n",
    "SCORE_REVIDS                  = '''SELECT revid FROM feature_vector WHERE revid > 646706890 ORDER BY revid LIMIT 4000'''\n",
    "\n",
    "VERSION_MODEL                 = 'v3'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "create_sqlite_tables()\n",
    "# export_tsv_to_sqlite()\n",
    "# download_conents()\n",
    "# extract_features()\n",
    "# copy_other_features_to_features_db()\n",
    "# build_model_wihout_other_features()\n",
    "# score_model_iterative(consider_other_features = False)\n",
    "# example_predictions()\n",
    "# gbc = process_build_model(threshold = 0.00435)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_e4da3b7fbbce2345d7772b0674a318d5.pkl\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sabya/env/3.4/lib/python3.4/site-packages/sklearn/feature_selection/base.py:80: UserWarning: No features were selected: either the data is too noisy or the selection test too strict.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct Predictions:  3594 Total Predictions:  3728 Score:  96.40557939914163\n",
      "average precision score:  0.397572508667 roc auc score:  0.910173520552\n"
     ]
    }
   ],
   "source": [
    "# selected features = 0\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold = 5)\n",
    "process_evaluate(gbc, threshold = 5)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_9518d2900743911eca1c02195b0ce0a4.pkl\n",
      "Not found. Creating..\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "  model_pickled/labels_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "Length of get_support: 50\n",
      "Shape of combined features:  (16000, 127)\n",
      "fitting\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 50\n",
      "Correct Predictions:  3596 Total Predictions:  3728 Score:  96.45922746781116\n",
      "average precision score:  0.371622175134 roc auc score:  0.906081399713\n"
     ]
    }
   ],
   "source": [
    "# selected features = 50\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold = 0.00435)\n",
    "process_evaluate(gbc, threshold = 0.00435)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_f217ebb62a283d92c7b487855807b225.pkl\n",
      "Not found. Creating..\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "  model_pickled/labels_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "Length of get_support: 71\n",
      "Shape of combined features:  (16000, 148)\n",
      "fitting\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 71\n",
      "Correct Predictions:  3592 Total Predictions:  3728 Score:  96.35193133047211\n",
      "average precision score:  0.355803409751 roc auc score:  0.904444358898\n"
     ]
    }
   ],
   "source": [
    "# selected features = 75\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold = 0.00365)\n",
    "process_evaluate(gbc, threshold = 0.00365)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_866ec452137b6ee7f2f4ca80f0f09f7d.pkl\n",
      "Not found. Creating..\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "  model_pickled/labels_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "Length of get_support: 108\n",
      "Shape of combined features:  (16000, 185)\n",
      "fitting\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 108\n",
      "Correct Predictions:  3590 Total Predictions:  3728 Score:  96.29828326180258\n",
      "average precision score:  0.363625583156 roc auc score:  0.902835227655\n"
     ]
    }
   ],
   "source": [
    "# selected features = 100\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold=0.0028)\n",
    "process_evaluate(gbc, threshold=0.0028)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_df353306f7483e0e926e83d342bccd76.pkl\n",
      "Not found. Creating..\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "  model_pickled/labels_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "Length of get_support: 262\n",
      "Shape of combined features:  (16000, 339)\n",
      "fitting\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 262\n",
      "Correct Predictions:  3590 Total Predictions:  3728 Score:  96.29828326180258\n",
      "average precision score:  0.331128436243 roc auc score:  0.895407431646\n"
     ]
    }
   ],
   "source": [
    "# selected features = 250\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold=0.00117225)\n",
    "process_evaluate(gbc, threshold=0.00117225)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trying to load pickle files:\n",
      "  model_pickled/model_threshold_v3_b187dce5727693e5793dfa13e64f350f.pkl\n",
      "Not found. Creating..\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "  model_pickled/labels_8abc6fb50e4c53022db7dc96a8ba9e65.pkl\n",
      "Length of get_support: 776\n",
      "Shape of combined features:  (16000, 853)\n",
      "fitting\n",
      "trying to load pickle files:\n",
      "  model_pickled/hv_features_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "  model_pickled/labels_64077a5b8001403b1b92979078ab65c8.pkl\n",
      "Length of get_support: 776\n",
      "Correct Predictions:  3591 Total Predictions:  3728 Score:  96.32510729613733\n",
      "average precision score:  0.351808104154 roc auc score:  0.893947472259\n"
     ]
    }
   ],
   "source": [
    "# selected features = 500\n",
    "clean_score_table()\n",
    "gbc = process_build_model(threshold = 0.000085)\n",
    "process_evaluate(gbc, threshold = 0.000085)\n",
    "calculate_performance_stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {
    "autoscroll": "json-false",
    "collapsed": false,
    "ein.tags": [
     "worksheet-0"
    ],
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'init': None,\n",
       " 'learning_rate': 0.01,\n",
       " 'loss': 'deviance',\n",
       " 'max_depth': 7,\n",
       " 'max_features': None,\n",
       " 'max_leaf_nodes': None,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 700,\n",
       " 'presort': 'auto',\n",
       " 'random_state': None,\n",
       " 'subsample': 1.0,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbc.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  },
  "name": "poc_hashing_vectorizer.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
